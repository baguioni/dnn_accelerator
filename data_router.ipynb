{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb014e4e-7a78-4be8-8930-ea6ade1aefae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "N = 1   # Batch size\n",
    "K = 1   # Number of filters (output channels)\n",
    "C = 3   # Input channels\n",
    "W = 5   # Output width\n",
    "H = 5   # Output height\n",
    "R = 3   # Kernel/filter height\n",
    "S = 3   # Kernel/filter width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fb262e8-9ef6-426b-8afb-0170d4f1cc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_tensor(N, C, W, H):\n",
    "    return np.array(np.random.randint(0, 255, (N, C, W, H), dtype=np.uint8))\n",
    "\n",
    "def create_kernel_tensor(K, C, R, S):\n",
    "    # Filter dimensions: (K, C, R, S)\n",
    "    filter_tensor = np.zeros((K, C, R, S), dtype=np.uint8)\n",
    "    \n",
    "    # Fill the filter with identity matrices (1s along the diagonal)\n",
    "    for k in range(K):\n",
    "        for c in range(C):\n",
    "            for i in range(min(R, S)):  # Ensure it's square (R == S)\n",
    "                filter_tensor[k][c][i][i] = 1\n",
    "    return np.array((filter_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe34b3ad-5d94-4a42-857d-29a4052184b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_router(ABC):\n",
    "    def __init__(self, computation_unit, accumulator):\n",
    "        self.computation_unit = computation_unit\n",
    "        self.accumulator = accumulator\n",
    "\n",
    "    def compute_parameters(self, ifmap, kernel):\n",
    "        N, C, W, H = ifmap.shape\n",
    "        K, _, R, S = kernel.shape\n",
    "\n",
    "        # Ignore padding and strides for now\n",
    "        W = W-S+1 # Output width\n",
    "        H = H-R+1 # Output height\n",
    "        return (N, K, C, W, H, R, S)\n",
    "\n",
    "    @abstractmethod\n",
    "    def execute(self):\n",
    "        pass\n",
    "\n",
    "# Each individual PE represents a unit similar to a Fusion Unit. \n",
    "# In other words, it is composed of 16 Bitbricks and performs 8-bit by 8-bit multiplication\n",
    "class pe_array:\n",
    "    def __init__(self, width, height):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "    # fmap and weight can be shaped depending on algorithm for implementing cnn\n",
    "    def compute_psum(self, fmap, weight):\n",
    "        psums = []\n",
    "        for r in range(self.width):\n",
    "            for s in range(self.height):\n",
    "                psums.append(fmap[r][s] * weight[r][s])\n",
    "        return psums\n",
    "\n",
    "# Accumulator is dependent on algorithm and shape of pe_array\n",
    "class accumulator_1d:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def accumulate(self, psums):\n",
    "        return sum(psums)\n",
    "\n",
    "class sliding_window_single_channel(data_router):\n",
    "    def execute(self, ifmap, kernel):\n",
    "        cycles = 0\n",
    "        ifmap = np.array(ifmap)\n",
    "        kernel = np.array(kernel)\n",
    "        \n",
    "        N, K, C, W, H, R, S = self.compute_parameters(ifmap, kernel)\n",
    "\n",
    "        # Output dimensions: (N, K, W, H)\n",
    "        output = np.zeros((N, K, W, H), dtype=np.uint8)\n",
    "\n",
    "        # Perform the convolution operation (iterating over the 7 nested loops)\n",
    "        for n in range(N): # Loop over batch size\n",
    "            for k in range(K): # Loop over output channels (filters)\n",
    "                for c in range(C): # Loop over input channels\n",
    "                    for w in range(W): # Loop over output width\n",
    "                        for h in range(H): # Loop over output height\n",
    "                            # Extract the sliding window of the feature map\n",
    "                            rows = ifmap[n][c][w:w+S]\n",
    "                            sliding_window = []\n",
    "                            for row in rows:\n",
    "                                sliding_window.append(list(row[h:h+R]))\n",
    "        \n",
    "                            psums = self.computation_unit.compute_psum(sliding_window, kernel[k][c])\n",
    "                            accumulated_results = self.accumulator.accumulate(psums)\n",
    "                            output[n][k][w][h] = accumulated_results\n",
    "\n",
    "                            cycles += 1\n",
    "        return (output, cycles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6dda7ad8-5df0-4002-8066-4e2e36d03e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class accumulator_2d:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def accumulate(self, psums):\n",
    "        final_sum = 0\n",
    "\n",
    "        for psum in psums:\n",
    "            final_sum += sum(psum)\n",
    "        return final_sum\n",
    "        \n",
    "# Assume tensor elements has same number of input channels\n",
    "class sliding_window_multi_channel(data_router):\n",
    "    def execute(self, ifmap, kernel):\n",
    "        cycles = 0\n",
    "        ifmap = np.array(ifmap)\n",
    "        kernel = np.array(kernel)\n",
    "        \n",
    "        N, K, C, W, H, R, S = self.compute_parameters(ifmap, kernel)\n",
    "\n",
    "        # Output dimensions: (N, K, W, H)\n",
    "        output = np.zeros((N, K, W, H), dtype=np.uint8)\n",
    "\n",
    "        # Perform the convolution operation (iterating over the 7 nested loops)\n",
    "        for n in range(N): # Loop over batch size\n",
    "            for k in range(K): # Loop over output channels (filters)\n",
    "                    for w in range(W): # Loop over output width\n",
    "                        for h in range(H): # Loop over output height\n",
    "                            pe_input = []\n",
    "                            # Extract the sliding window of multiple channels the feature map\n",
    "                            for c in range(C): \n",
    "                                rows = ifmap[n][c][w:w+S]\n",
    "                                sliding_window = []\n",
    "                                for row in rows:\n",
    "                                    sliding_window.append(list(row[h:h+R]))\n",
    "                                pe_input.append(sliding_window)\n",
    "\n",
    "                            # Spatially unroll input channels or tensor elements\n",
    "                            # Frankly, this is not necessary since python wont execute this in parallel\n",
    "                            # but it should visualize the architecture\n",
    "                            psum_channel_0 = self.computation_unit.compute_psum(pe_input[0], kernel[k][0])\n",
    "                            psum_channel_1 = self.computation_unit.compute_psum(pe_input[1], kernel[k][1])\n",
    "                            psum_channel_2 = self.computation_unit.compute_psum(pe_input[2], kernel[k][2])\n",
    "                            accumulated_results = self.accumulator.accumulate([psum_channel_0, psum_channel_1, psum_channel_2])\n",
    "                            output[n][k][w][h] = accumulated_results\n",
    "\n",
    "                            cycles += 1\n",
    "        return (output, cycles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bc4bbc35-9afa-4e38-b4fd-8ce26094cac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 36  48 231]\n",
      "   [ 52  65 148]\n",
      "   [105 145 143]]]] 9\n"
     ]
    }
   ],
   "source": [
    "input_tensor = create_input_tensor(N, C, W, H)\n",
    "#print(input_tensor)\n",
    "kernel_tensor = create_kernel_tensor(K, C, R, S)\n",
    "#print(kernel_tensor)\n",
    "\n",
    "# Single-channel of kernel fully maps to PE array\n",
    "#pe = pe_array(3,3)\n",
    "#accu = accumulator()\n",
    "#sliding_window = sliding_window_single_channel(pe, accu)\n",
    "#output = sliding_window.execute(input_tensor, kernel_tensor)\n",
    "#print(output)\n",
    "\n",
    "# Multiple channels of kernel maps to PE array\n",
    "accu_unit = accumulator_2d()\n",
    "pe = pe_array(3,3)\n",
    "sliding_window_multi_channel = sliding_window_multi_channel(pe, accu_unit)\n",
    "output, cycles = sliding_window_multi_channel.execute(input_tensor, kernel_tensor)\n",
    "print(output, cycles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5b73de-ee01-4ded-bbe7-899e5f0e7c34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
